{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66OM6FlIi7xc"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yZWLPThizJr"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install portaudio19-dev #Often required for PyAudio.\n",
        "!pip install SpeechRecognition pydub PyAudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdKXtnHjykT"
      },
      "source": [
        "# Speech Recognition\n",
        "\n",
        "### Listens to the user, and then recognizes the stuffs to store in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDPcNZFgi1ZP"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def beep_begin():\n",
        "    if sys.platform == \"win32\": # If system is Windows\n",
        "        import winsound\n",
        "        frequency = 1000  # Set Frequency To 1000 Hertz\n",
        "        duration = 500    # Set Duration To 500 ms == 0.5 second\n",
        "        winsound.Beep(frequency, duration)\n",
        "    else:\n",
        "        os.system('echo -e \"\\a\"') # If system is Linux/Mac, give a terminal beep.\n",
        "\n",
        "def beep_end():\n",
        "    if sys.platform == \"win32\": # Same thing as above\n",
        "        import winsound\n",
        "        frequency = 4000  # Set Frequency To 4000 Hertz now\n",
        "        duration = 500\n",
        "        winsound.Beep(frequency, duration)\n",
        "    else:\n",
        "        os.system('echo -e \"\\a\"')\n",
        "\n",
        "def save_text_to_file(text, filename=\"transcription.txt\"):\n",
        "    with open(filename, \"a\") as file:\n",
        "        file.write(text + \"\\n\")\n",
        "\n",
        "def get_speech_input():\n",
        "    recognizer = sr.Recognizer()\n",
        "    mic = sr.Microphone()\n",
        "\n",
        "    beep_begin()  # Beep before listening\n",
        "    print(\"Listening...\")\n",
        "\n",
        "    with mic as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    beep_end()  # Beep after listening\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        print(f\"You said: {text}\")\n",
        "        save_text_to_file(text) # The text is stored in file named transcription.txt\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "get_speech_input()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ukdrj_djjWE"
      },
      "source": [
        "# NLP Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAosvT3_8wVm"
      },
      "source": [
        "Reading the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "mbtRxyomI2s9"
      },
      "outputs": [],
      "source": [
        "#Read the file\n",
        "with open(\"transcription.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Empty the file after read\n",
        "with open(\"transcription.txt\", \"w\") as file:\n",
        "    file.write('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U6ngO-7808_"
      },
      "source": [
        "Getting the list of symptoms so we can basically handle multiword stuffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf0Nxgx8nYlG",
        "outputId": "10a5655f-cfe6-445f-a461-bde95d207d53"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Select all columns except the first one (assuming 'Disease' is the first column in my case ).\n",
        "# Basically, Select the columns with symptoms\n",
        "symptom_columns = df.columns[1:]\n",
        "\n",
        "symptoms = pd.concat([df[col] for col in symptom_columns]).unique()\n",
        "\n",
        "symptoms = [s.replace('_', ' ') if isinstance(s, str) else s for s in symptoms]\n",
        "\n",
        "symptoms = [s for s in symptoms if isinstance(s, str)]\n",
        "\n",
        "symptoms = [s.strip() for s in symptoms]\n",
        "\n",
        "print(symptoms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je3B-R8E9AGp"
      },
      "source": [
        "Now we do tokenization !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us1NZ3ensQ2a",
        "outputId": "932ac416-d57c-4172-9765-bb0ffe3c8b64"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "import re\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def custom_tokenize(text, symptoms, tokenizer):\n",
        "    # Prepare regex patterns to match symptoms\n",
        "    pattern = re.compile('|'.join([re.escape(symptom) for symptom in symptoms]), re.IGNORECASE)\n",
        "\n",
        "    # Replace multi-word symptoms with placeholders\n",
        "    tokens = []\n",
        "    for match in pattern.finditer(text):\n",
        "        start, end = match.span()\n",
        "        tokens.append(text[start:end])\n",
        "\n",
        "    # Tokenize the rest of the text\n",
        "    remaining_text = pattern.sub('', text)\n",
        "    remaining_tokens = tokenizer.tokenize(remaining_text)\n",
        "\n",
        "    return tokens + remaining_tokens\n",
        "\n",
        "tokens = custom_tokenize(text, symptoms, tokenizer)\n",
        "\n",
        "tokens = [token for token in tokens if token not in string.punctuation]\n",
        "print(tokens)\n",
        "\n",
        "def replace_spaces_with_underscores(tokens):\n",
        "    return [token.replace(' ', '_') for token in tokens]\n",
        "\n",
        "# Replace spaces with underscores once again.\n",
        "tokens_with_underscores = replace_spaces_with_underscores(tokens)\n",
        "\n",
        "print(tokens_with_underscores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mFID7M2zkt"
      },
      "source": [
        "# Disease Identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UemKbzUk1dlX",
        "outputId": "47cc62c4-40e4-4d0b-cb89-72faf0cdaa80"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "def find_highest_probability_disease(df, symptoms):\n",
        "    disease_match_count = {}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        disease = row['Disease']\n",
        "        disease_symptoms = set([str(row[col]).strip().lower() for col in df.columns if col != 'Disease' and pd.notna(row[col])])\n",
        "\n",
        "        # Calculate the number of matching symptoms\n",
        "        matching_symptoms = symptoms.intersection(disease_symptoms)\n",
        "        match_count = len(matching_symptoms)\n",
        "\n",
        "        if match_count > 0:\n",
        "            disease_match_count[disease] = (match_count, matching_symptoms)\n",
        "\n",
        "    sorted_diseases = sorted(disease_match_count.items(), key=lambda item: item[1][0], reverse=True)\n",
        "\n",
        "    return sorted_diseases\n",
        "\n",
        "# Convert tokens to a set of symptoms for comparison\n",
        "symptoms_set = set(tokens)\n",
        "\n",
        "matching_diseases = find_highest_probability_disease(df, symptoms_set)\n",
        "\n",
        "if matching_diseases:\n",
        "    for disease, (count, relevant_symptoms) in matching_diseases:\n",
        "        print(f\"Disease: {disease}, Matching Symptoms: {count}\")\n",
        "\n",
        "    top_disease, (top_count, top_relevant_symptoms) = matching_diseases[0]\n",
        "    top_relevant_symptoms_str = ', '.join(top_relevant_symptoms)\n",
        "\n",
        "    print(f\"\\nDisease with highest matching symptoms: {top_disease}\")\n",
        "    print(f\"Matching Symptoms: {top_relevant_symptoms_str}\")\n",
        "\n",
        "    if len(matching_diseases) > 3:\n",
        "        print(\"\\n\\nA lot of diseases are identified for further checkups.\\nTests are preferred or doctor consultation is preferred.\")\n",
        "else:\n",
        "    print(\"No matching disease found. Proceed to doing tests instead of symptomatic treatment.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
